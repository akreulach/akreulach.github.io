\documentclass[]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{plain}                                                      %%
%%%%%%%%%% EXACT 1in MARGINS %%%%%%%                                   %%
\setlength{\textwidth}{6.5in}     %%                                   %%
\setlength{\oddsidemargin}{0in}   %% (It is recommended that you       %%
\setlength{\evensidemargin}{0in}  %%  not change these parameters,     %%
\setlength{\textheight}{8.5in}    %%  at the risk of having your       %%
\setlength{\topmargin}{0in}       %%  proposal dismissed on the basis  %%
\setlength{\headheight}{0in}      %%  of incorrect formatting!!!)      %%
\setlength{\headsep}{0in}         %%                                   %%
\setlength{\footskip}{.5in}       %%                                   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                   %%
\newcommand{\required}[1]{\section*{\hfil #1\hfil}}                    %%
\renewcommand{\refname}{\hfil References Cited\hfil}                   %%
\bibliographystyle{plain}                                              %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{cite}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{float}

\begin{comment}
    \begin{figure}[H]
        \includegraphics[width=300pt]{pictureFileName}
        \caption{Example Image}
        \centering
    \end{figure}
\end{comment}

%opening
\title{\huge Celestial-Inertial UAS Navigation in a GPS-negative Environment\\\tiny ~\\
        \large Auburn REU on SMART UAVs 2018\\
        \large Technical Report \#CSSE18-05}
\author{
  Austin Kreulach\\
  \texttt{University of Arkansas}\\
  \texttt{apkreula@uark.edu}
  \and
  Harrison Welch\\
  \texttt{Austin Peay State University}\\
  \texttt{hwelch1@my.apsu.edu}
  \and
  Dr. Richard Chapman\\
  \texttt{Auburn University}\\
  \texttt{chapmro@auburn.edu}
  \and
  Dr. Saad Biaz\\
  \texttt{Auburn University}\\
  \texttt{biazsaa@auburn.edu}
  }
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
	As technology continues to advance, electronics become cheaper and batteries smaller. This has led to cheaper and larger drones taking on new and more complex roles beyond their traditional place as either military hardware or hobbyist toy. As drones increase in both size and importance, we have to look more carefully at safety. Most drones, when guided autonomously, depend on the GPS network to function. GPS depends on relatively weak signals and is exceptionally easy to “jam” or render inoperable. This project aims to provide a passive and reliable method of Unmanned Aerial System astronavigation using careful calculations of the stars around the observing aircraft(s) to determine precise position, while using inertial dead-reckoning system for correcting the celestial computations. This method will provide a passive method of determining location precisely around the globe at low cost during the night.
\end{abstract}.

\section{Introduction}

    \begin{comment}
            Dr. Biaz's general recommendations
            7) Do not put dry numbers as references, accompany the citations with the names of the authors.
    \end{comment}
    
    An Unmanned Aerial Vehicle \emph{(UAV)}, or drone, is a device intended to fly without a pilot. These devices can be autonomous, semi-autonomous, or radio-controlled. Ultimately a UAV can still be piloted by a human, the main stipulation is that no control staff be physically aboard the craft.\newline
    
    With the increasing economic viability of UAV technology, drones have taken a larger role in various fields from delivery to search-and-rescue to military applications. These advances have led to less interaction from drone operators, longer missions, and a greater reliance on autonomous systems. This has naturally introduced new challenges in using this technology safely.\newline
    
    One of the main concerns in the development of modern drones is the ability to navigate accurately and precisely. Apart from the need to avoid collisions with both stationary objects and other craft sharing the airspace, a drone must be able to know its location in order to find a course to the target area. The most immediate and common solution to this is to use a GPS module on the UAV to handle all necessary localizing tasks. However, a great deal of work has gone into finding alternatives to GPS navigation due to the weak signal and accompanying problems.\newline
    
    The current alternatives to GPS navigation suffer from various problems including cost, bulk, speed, lack of universal applicability, or accuracy. Magnetometry, for example, demands fifty thousand dollars worth of specialist equipment on a seven foot boom extending from the back of the aircraft, but is otherwise very precise and successful. The problems with most methods can be solved with more advanced components or better implementation, but there might be a simpler, cheaper and more effective solution available. \newline
    
    The goal of this paper is to develop a system for determining location precisely using celestial navigation and computer vision. Having introduced the basic background and motivation of this investigation, these ideas will be explored in greater depth in the following sections in addition to current work relating to this paper and topic before finally explaining the results of this investigation and what value can be derived therein.\newline

\section{Problem Statement}
    The problem that inspired this paper was simple: a drone in the field has lost the ability to effectively use the global positioning system \emph{(GPS)} and it needs a way to get home. The reason it lost GPS is irrelevant, but there are a great many ways this system can fail. GPS is dependent on a very weak signal, meaning that it takes very little power to jam the signal over a wide area. For example, the United States military is developing a system called NAVWAR that, among other things, can block GPS signals in a five hundred mile radius. To give that figure a slightly greater impact, the entirety of modern France can be contained in one such system [citation needed]. A single plane with a one watt jamming device can disconnect devices that have already made contact with a satelite within roughly seven miles, and it can prevent connection within a nearly forty miles radius [citation needed]. This military capacity means that GPS cannot be relied upon in warzones or even in areas with mildly frosty relations due to the simplicity and indirect nature of the jamming operation.\newline
    
    What makes this problem worse is that even civilians have formidable jamming capabilities. Jamming devices are currently illegal to sell or use in the United States, but it is not uncommon to find dead zones where jamming devices are in use. Consumer GPS jammers have a much smaller range than their military counterparts, usually ranging between a dozen feet and a mile [citation needed]. Common reasons for finding holes in the GPS network include truckers trying to avoid tracking devices, undercover police officers, and hobbyists trying to avoid GPS-locked aviation guidelines [citation needed]. As a result of these shadows in the network, both drones and planes are required to have some way of avoiding collisions and determining place that does not depend solely on the GPS network.\newline
    
    The proposed method, astronavigation, is the analysis of the stars to determine position. A common abstraction in astronavigation is to assume that all stars visible from earth are actually points on a celestial sphere that rotates around an axis shared with the Earth. This abstraction works largely because the stars are far enough away that despite the Earth's motion relative to them, their perceived direction(position in the night sky) does not change noticeably. Given this model, the stars will still rotate over the surface of the Earth. This means that only the poles will have a constant set of stars that can be seen overhead from month to month. However, if you bring a further constraint to the problem then the stars only appear in particular places on the celestial sphere over specific locations on Earth. That constraint is time. If the exact time is known at a specific point of longitude, Greenwich, England for example, then functionally the celestial sphere no longer rotates. The problem of identifying location from the stars becomes one of identifying which stars are above and how they relate to the viewer. \newline
    
    For example, if an observer knows that it is precisely 8:30 pm on April 16th 1954 in Greenwich, and the observer also identifies Polaris is fifteen degrees, six arcminutes and forty arcseconds above the horizon, then the observer's exact latitude and longitude coordinates can be determined.\newline
    
    A final line of thinking on the problem is the question of precision. As might be guessed, astronavigation depends strongly on extremely precise and accurate measurement of a very small point very far away. The previous paragraph mentions arcminutes, which are roughly one sixtieth of a degree. If a measurement is wrong by one arcminute, then the resulting estimate of position is wrong by roughly one nautical mile. This means that very small and usually negligible sources of error are magnified by incredible factors in the context of this problem. Factors like the wind tilting an aircraft or a gyroscope drifting half a degree become critical failures in the subsystem.\newline

\section{Literature Review}
    Historically, aviation has depended greatly on the skills of the pilot. This increased distance between pilot and craft had led to new challenges in introducing UAVs into civil airspace. While prior FAA regulations required the pilot to simply \emph{see and avoid} other aircraft so as to allow flight without modern instruments, UAVs cannot 'see' in any legal sense. Thus, they must \emph{sense and avoid}. The primary focus of UAV research is, then, collision avoidance and navigation. The primary navigational method is GPS, due to its exceptional coverage, precision, affordability, low weight, and compact size. However, GPS can frequently be unreliable. In both the civil and military airspace, GPS is often jammed, spoofed, or otherwise tampered with \cite{tag1} to the point of being an unacceptable point of failure. Despite this problem, GPS does remain the gold standard of navigational aids. Many methods have been proposed, tested, and implemented as solutions to the problem of non-GPS navigation. \newline
	
	The first and most obvious solution is to use exceptionally accurate and precise tools to measure the forces on a craft and use that to calculate the current location using a previous known location. This is known as {\em dead reckoning}. As the name implies it is usually met with little success. However, some recent papers \cite{tag2} have shown careful use of math and new technology can present decent results even with this method. The dead reckoning system, or inertial navigation system (INS) was deployed on a person's ankle and used to determine their position after a walk. The potential applications of this sort of method to aerial vehicles is more suspect due to the potential for unpredictable wind and other complex forces on the aircraft. \newline
	
    Another approach to this problem is Light Detection and Ranging or {\em LiDAR}, which originated as a portmanteau of light and RADAR ({\em Radio Detection and Ranging}) but eventually grew into a backronym. More to the point, LiDAR depends on a series of pulsed light emissions being measured at emission and receipt, then using the minute differences to calculate range. That is, LiDAR is a large bundle of laser range-finders being used in conjunction to create a 3D map of an area.\newline
    	
    The primary draw of LiDAR is its ability to penetrate thin surfaces, allowing the device to map the areas below thin cover. This is largely used in forestry surveys \cite{tag3} due to the exceptional performance with leaves. The system does not offer much promise as a primary drone navigational technique, however, due to the active nature. As an active system, LiDAR sends out pulses which can be easily interfered with or detected. This poses a challenge in hostile environments.\newline
    
    A slightly less popular but nonetheless fascinating method is navigation by infrasound. This is believed to be the navigation method used by homing pigeons \cite{tag4} and as a result shows great promise as a potential navigational solution usable from almost anywhere on earth. The concern is precision, however. As the cited paper shows, infrasound is difficult to measure precisely and it is further difficult to discern significant infrasound signals from noise. Thus far there has been no significant success using infrasound to navigate, at least none that this team has managed to find.\newline
    
    Another option investigated largely for its unique and fascinating qualities is \emph{optical flow}. Optical flow is a subset of computer vision where two images are taken at a specific time step apart and then analyzed to determine differences in color, pixel density, and other properties to show change over time. This is used mainly to determine if an object in the image has moved closer to the perspective or further away. What makes optical flow interesting is how easy it is to program autonomous routines using this information. Take for example a region of the image is moving closer at a rate faster than other parts of the image. If the goal is collision avoidance, then simply turn away from that side of the camera. If the goal is pursuit of a target, then turn toward that region to keep it in focus. If the goal is avoiding pursuit, then turn toward the area of motion and drive backward\cite{tag5}. It is incredible how simple and diverse the uses of this information is. However, ease of coding is not the highest priority in drone navigation, and as such the other problems with this technique including high computational requirements tend to leave it as a poor choice.\newline
    
    Another entry in the series of navigational methods chosen for interesting properties is navigation by signals of opportunity. The method is fundamentally answering the problem of lacking a GPS signal by asking what other signals are available. A signals of opportunity navigation system is composed of a great many receivers for various signals such as radio or television broadcasts, and then using known signal emitters such as towers or stations to triangulate position. This is relatively expensive and bulky \cite{tag6}, however, and is further limited to urbanized areas that have a sufficiently high density of stray signals. It is thus unfit for use on the ocean rural areas of undeveloped countries.\newline
    
    A modern way of tackling the issue of non-GPS navigation would be the use of machine learning or fingerprint navigation. This generally involves two phases of action: Training and Comparison. The training phase generally consists of running the flying instrument through environments akin to the actual area of action the UAS will actually be flying through. Fingerprints are the data the UAS uses to navigate and over time running many training flights the collection of fingerprints grows and so does the accuracy. These fingerprints can be gathered in a multitude of ways from received signal strength indicators (RSSI), computer vision, and/or magnetometry among other things \cite{tag7}\cite{tag13}. Once the training phase is complete/sufficient then the comparison or implementation phase begins. This again can be implemented many ways such as a nearest neighbor algorithm to find the truest position based off the current assumed/probable location of the candidate signal. \cite{tag7} explores this nearest neighbor approach under an indoor situation detecting active wifi and cell phone signals. Other strategies of the comparison phase could be the neural network approach. Neural Networks make probabilistic decisions based off of a database of trained data in groups. Similarly the navigator or UAV would use its trained data to analyze groups of passively detected points to come to the best made decision. All said, fingerprint navigation implementations offer an interesting way for a UAV to travel.\newline
    
    The United States Air Force Institute of Technology in Ohio has preformed a study of how Magnetometry can be used to navigate \cite{tag8}. They used a previously produced magnetic anomaly map of about 35 square kilometers from 2012 to fly a turbine Geo-survey Then during the test flight the plane was able to simply match the magnetic readings of the map to the ones the plane was passively detecting from the outside. One issue the authors mentioned was that the plane must fly fairly close to the ground for the best accuracy. While the paper did not mention exactly how low, the authors expressed low altitudes are better many times in the text. Another issue is the weather can change magnetic signals picked up by the sensors. This will need to be compensated for either by attaching the required sensors to the flying instrument or by actively sending them up to the flying instrument from the ground, both options of which add to the weight and cost of the overall system. Cost is always a factor in problem solving, the Air Force used what are called "optically pumped scalar magnetometers" which costs in the range of \$15,000 to \$25,000 putting it far outside the financial range of the average civilian. Cost and low altitude aside the magnetometry method yields excellent results with only 13 meter errors in true location.\newline
    
    An interesting study out of the University of Alabama Huntsville uses the Panoramic annular lens attitude determination system or PALAD \cite{tag9}. Implemented on a high altitude satellite, this system uses a special lens that reflects and refracts light to create an 360 degree photo or panorama of the satellite's surroundings at about 45 degrees in angle height-wise. This system enables the camera (with the lens attached) to see all the way around its current position. The paper uses this panorama to determine the attitude in the satellite by calculating the position of the earth in the photograph. The lens style approach seems promising, assuming the altitude requirement is not too extreme.\newline
    
    Filtering, when used for navigation purposes, removes any unwanted data and provides a better estimation of the user's position. According to \cite{tag7}, there are at least three major types of filtering used for navigation: Kalman, Exponential Weighted Moving Average (EWMA), and Linear Weighted Moving Average (LWMA). Kalman filters use matrix multiplication to remove any noise, minor or major, from a data set over time. EWMA serves the same purpose as the Kalman filter, which is to remove unwanted noise, but this method saves some computing time at the cost of some accuracy. If applied in the right context EWMA may out-preform the Kalman filter if the accuracy is lesser needed. Simliar to the previous two methods, LWMA again serves to filter out noise from a data set while saving time computation-wise. \cite{tag7} uses LWMA to remove any errors when measuring the wifi signal strengths surrounding the navigating UAS, and \cite{tag12} uses a form of Kalman filter called "Memory fading unscented Kalman filter" where they put more weight on recent data estimations and less weight on older estimations.\newline
    
    This paper plans to advance a system that uses a top-mounted camera and an accurate clock to determine location at any point on Earth at night. The reason the camera is mounted rather than in a gimbal is to reduce the potential for gyroscopic error. Curiously, if this method is expanded to include dusk, dawn, and day astronavigation by the sun then the mounting structure would require a gimbal camera mount. Given the assumption that the system only works on cloudless nights, however, a rigid mount provides additional precision. Which, as mentioned earlier in this paper, can mean cutting the margin of error down by as much as half a mile. Another benefit of the rigid mount is the simplifying assumption that all images are centered on the zenith. Zenith is an astronomical concept meaning endpoint on the celestial sphere of a line normal to the observer's tangent plane on the terrestrial sphere. That is, the point directly overhead.\newline
    
    Given this zenith-centered image of the starfield above, the image can be fed into the desktop application version of Astrometry.net \cite{tag11}. Astrometry.net is an ongoing project that analyzes images of starfields and identifies the individual stars in the image, in addition it identifies the section of the celestial sphere shown in the image. This information can be combined with the absolute local time provided by a basic digital clock to find the exact position on Earth that the image was taken. This step is the primary focus of this paper and represents the contribution of the research team to the field. \newline
    
    One potential approach to this problem is through use of the astronomy-based Python library, PyEphem. The library has several functions for analyzing the relationship between an observer and a heavenly object. A challenge of using this approach is that the library is designed for amateur or professional astronomers rather than navigators, and as such has excellent support for finding what is visible to a given observer, but is less able to handle the problem of identifying potential observers given a constrained celestial object. PyEphem will act not as the actual tool for solving this problem but rather as a framework for the solution.\newline

\newpage
\bibliography{bib1}{}

\end{document}